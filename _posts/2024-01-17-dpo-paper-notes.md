---
layout: single
title:  "Direct Preference Optimization: Your Language Model is Secretly a Reward Mode"
classes: wide

categories:
  - notes

tags:
  - paper
  - llm
  - fine-tunning
  - alignment
mathjax: true
---
RLHF is used for aligning language models with human preferences. However, RLHF is a complex and often unstable procedure. 
Proposed algorithm DPO is stable, performant and computationally lightweight. It does not require sampling from the model during fine-tunning or performing significant hyper-parameter tunning.
The algorithm tries to solve the RLHF problem with only a simple classification loss which made possible with a new parameterization of the reward model. 

## Introduction

We may need our language model to recognize common programming mistakes but we do not want it to generate incorrect code. 
It is important to select desired responses from models wide knowledge base.

RLHF is the most successful class of methods. RLHF involves training multiple LLMs and sampling from the LM policy in the loop of training, incurring significant computational costs.

Proposed method, DPO, does not require explicit reward modelling or reinforcement learning. DPO optimizes the same objective as RLHF (which is reward maximization with KL divergence constraint).

Intiutively, DPO update increases relative log probability of a preferred response to rejected responses. It incorporates a dynamic per-example importance weight to avoid model degeneration.

## Preliminaries

### Reward Modelling Phase:

Prompt SFT model with prompt x to generate completion y1 and y2. Then humans rank y1 and y2. Preferred response being yw (winning) and dispreferred response being yl (losing).

Preferences are assumed to be generated by a latent model $$r^*(y, x)$$ which we do not have access to. Preferences can be modelled using e Bradley-Terry model, according to which human preference distribution p* can be written as:

![formulae-1]({{site.baseurl}}/assets/images/dpo-formulae-1.png)

Assuming access to a static dataset D that is sampled from p* it is possible to parameterize a reward model $$r_\phi(x,y)$$ and estimate parameters using maximum likelihood.
Approching as a binary classification problem, we have negative log-likelihood loss (Ïƒ is the logistic function):

![formulae-2]({{site.baseurl}}/assets/images/dpo-formulae-2.png)

In the context of language models, $$r_\phi(x,y)$$ is initiated from SFT model by adding a linear layer on top that produces a single scalar reward value. In order to have less variance on the generated reward, reward is normalized to have an expected value of 0.

### RL Fine-tuning Phase
RLHF objective:

![formulae-3]({{site.baseurl}}/assets/images/dpo-formulae-3.png)

where beta is parameter controlling the deviation from the base reference policy $$\pi_{ref}$$, which is the initial SFT model $$\pi^{SFT}$$. 
The constraint (KL divergence constraint) is important to:
- prevent model from deviating too far from the distribution where reward model is accurate. 
- maintaining generation diversity
- preventing mode collapse to single high-reward answers.

Due to discrete nature of language generation, this objective is not differentiable and is typically optimized using RL. 

Reward function is consructed in the following way and PPO is used to optimize. Note that KL divergence constraint term is expanded as difference of logs.

![formulae-3-1]({{site.baseurl}}/assets/images/dpo-formulae-3-1.png)

## Direct Preference Optimization

In the proposed approach, loss function over reward functions is transformed into loss function over policies. After this modification, it is no longer necessary to model a reward function. 
Human preferences are still used for optimization. In this approach, the policy network represents both the language model and the reward which is implicit. 

### Deriving the DPO objective

The optimal solution to the KL-constrained reward maximization objective in Equation 3 takes following form:

![formulae-4]({{site.baseurl}}/assets/images/dpo-formulae-4.png)

![formulae-4-1]({{site.baseurl}}/assets/images/dpo-formulae-4-1.png)

Where Z(X) is the partition function. 

In equation 4, take log of both sides and leave reward function alone.

![formulae-5]({{site.baseurl}}/assets/images/dpo-formulae-5.png)

Substituting the reparameterization in equation 5 into the preference model in equation 1, the partition function cancels, and now it is possible to express human preference probability in terms of only the optimal policy $$\pi^*$$ and reference policy $$\pi_{ref}$$. As a result, the optimal RLHF policy $$\pi^*$$ under the Bradley-Terry model satisfies the preference model:

![formulae-6]({{site.baseurl}}/assets/images/dpo-formulae-6.png)

Maximum likelihood objective for a parameterized policy $$\pi_\theta$$:

![formulae-7]({{site.baseurl}}/assets/images/dpo-formulae-7.png)

Equation 7 resembles equation 2 where reward based log-likelihood loss is defined. 

In equation 7 ![formulae-7-1]({{site.baseurl}}/assets/images/dpo-formulae-7-1.png) can be viewed as implicit reward function. 



### What does the DPO update do?




## References
1. [DPO paper](https://arxiv.org/pdf/2305.18290.pdf)
